# if(i>1) all.results <- rbind(all.results, testing.results)
}
# ))
all.horizons[[length(all.horizons)+1]] <- testing.results
# all.horizons[[length(all.horizons)+1]] <- all.results
print(paste("Completed", obj, days, f))
}
all.days[[length(all.days)+1]] <- all.horizons
# return(list(all.horizons))
}
if(length(all.days) < 6) {
print(paste("Error on", obj))
}
assign(paste0("all.days.nn.",obj), all.days)
save(list=paste0("all.days.nn.",obj), file=paste0("results/all-days-nn-",obj,".RData"))
rm(list=paste0("all.days.nn.",obj))
}
}
i <-1493
# Setup training and testing data for model
current.obs <- training.obs+f+i-1
train.xx <- matrix(as.numeric(obj.data[i:(current.obs-f),]), nrow=length(i:(current.obs-f)), byrow=FALSE)
train.yy <- matrix(as.numeric(obj.data[(i+f):current.obs, predict.col]), nrow=length(i:(current.obs-f)), byrow=FALSE)
n <- intersect(which(!is.na(train.yy)), which(!apply(train.xx,1,anyNA)))
train.xx <- train.xx[n,]
train.yy <- train.yy[n,]
test.xx <- matrix(as.numeric(obj.data[current.obs,]), nrow=1, byrow=FALSE)
test.yy <- matrix(as.numeric(obj.data[(current.obs+f), predict.col]), nrow=1, byrow=FALSE)
test.index <- index(obj.data[current.obs,])
# Train neural network model
diurnal.terms <- c(grep("cos", colnames(obj.data)), grep("sin", colnames(obj.data)))
nn.train.data <- cbind(train.yy, train.xx[,-diurnal.terms])
colnames(nn.train.data)[2:ncol(nn.train.data)] <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
fmla <- as.formula(paste0("train.yy ~ ",
paste(paste0("response.",1:ncol(train.xx[,-diurnal.terms])),collapse=" + ")))
nn <- neuralnet(fmla, data=nn.train.data, hidden=3,
act.fct = "logistic",
linear.output = TRUE,
stepmax=1e+06)
nn.train.data <- train.xx[,-diurnal.terms]
colnames(nn.train.data) <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.train.data)
pred.nn <- pred.nn$net.result
# Calculate neural network error
SSE <- mean((train.yy-pred.nn)^2)
SST <- mean((train.yy-mean(train.yy))^2)
Rsqu.nn <- 1-SSE/SST
RMSE.nn <- sqrt(SSE)
# Forecast neural network
nn.test.data <- matrix(test.xx[,-diurnal.terms], nrow=1)
colnames(nn.test.data) <- paste0("response.",1:length(test.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.test.data)
pred.nn <- pred.nn$net.result
testing.results <- data.frame("Test.time"= test.index,
"Actual.forecast" = as.numeric(test.yy),
"NN.forecast" = as.numeric(pred.nn),
"Training.R2.NN" = as.numeric(Rsqu.nn),
"Training.RMSE.NN" = as.numeric(RMSE.nn))
i <- 1507
# Setup training and testing data for model
current.obs <- training.obs+f+i-1
train.xx <- matrix(as.numeric(obj.data[i:(current.obs-f),]), nrow=length(i:(current.obs-f)), byrow=FALSE)
train.yy <- matrix(as.numeric(obj.data[(i+f):current.obs, predict.col]), nrow=length(i:(current.obs-f)), byrow=FALSE)
n <- intersect(which(!is.na(train.yy)), which(!apply(train.xx,1,anyNA)))
train.xx <- train.xx[n,]
train.yy <- train.yy[n,]
test.xx <- matrix(as.numeric(obj.data[current.obs,]), nrow=1, byrow=FALSE)
test.yy <- matrix(as.numeric(obj.data[(current.obs+f), predict.col]), nrow=1, byrow=FALSE)
test.index <- index(obj.data[current.obs,])
# Train neural network model
diurnal.terms <- c(grep("cos", colnames(obj.data)), grep("sin", colnames(obj.data)))
nn.train.data <- cbind(train.yy, train.xx[,-diurnal.terms])
colnames(nn.train.data)[2:ncol(nn.train.data)] <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
fmla <- as.formula(paste0("train.yy ~ ",
paste(paste0("response.",1:ncol(train.xx[,-diurnal.terms])),collapse=" + ")))
nn <- neuralnet(fmla, data=nn.train.data, hidden=3,
act.fct = "logistic",
linear.output = TRUE,
stepmax=1e+06)
nn.train.data <- train.xx[,-diurnal.terms]
colnames(nn.train.data) <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.train.data)
pred.nn <- pred.nn$net.result
# Calculate neural network error
SSE <- mean((train.yy-pred.nn)^2)
SST <- mean((train.yy-mean(train.yy))^2)
Rsqu.nn <- 1-SSE/SST
RMSE.nn <- sqrt(SSE)
# Forecast neural network
nn.test.data <- matrix(test.xx[,-diurnal.terms], nrow=1)
colnames(nn.test.data) <- paste0("response.",1:length(test.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.test.data)
pred.nn <- pred.nn$net.result
train.xx
diurnal.terms
nn.train.data
str(nn.train.data)
(total.obs-2*f-training.obs+1)
# for(i in 1:(total.obs-2*f-training.obs+1)) { # Faster to do this in an lapply loop
# testing.results <- do.call("rbind", lapply(1:(total.obs-2*f-training.obs+1), function(i) {
testing.results <- foreach(i=1490:(total.obs-2*f-training.obs+1),
.combine = 'rbind', .packages = c("xts","stats","neuralnet")) %dopar% {
# Setup training and testing data for model
current.obs <- training.obs+f+i-1
train.xx <- matrix(as.numeric(obj.data[i:(current.obs-f),]), nrow=length(i:(current.obs-f)), byrow=FALSE)
train.yy <- matrix(as.numeric(obj.data[(i+f):current.obs, predict.col]), nrow=length(i:(current.obs-f)), byrow=FALSE)
n <- intersect(which(!is.na(train.yy)), which(!apply(train.xx,1,anyNA)))
train.xx <- train.xx[n,]
train.yy <- train.yy[n,]
test.xx <- matrix(as.numeric(obj.data[current.obs,]), nrow=1, byrow=FALSE)
test.yy <- matrix(as.numeric(obj.data[(current.obs+f), predict.col]), nrow=1, byrow=FALSE)
test.index <- index(obj.data[current.obs,])
# Train neural network model
diurnal.terms <- c(grep("cos", colnames(obj.data)), grep("sin", colnames(obj.data)))
nn.train.data <- cbind(train.yy, train.xx[,-diurnal.terms])
colnames(nn.train.data)[2:ncol(nn.train.data)] <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
fmla <- as.formula(paste0("train.yy ~ ",
paste(paste0("response.",1:ncol(train.xx[,-diurnal.terms])),collapse=" + ")))
nn <- neuralnet(fmla, data=nn.train.data, hidden=3,
act.fct = "logistic",
linear.output = TRUE,
stepmax=1e+06)
nn.train.data <- train.xx[,-diurnal.terms]
colnames(nn.train.data) <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.train.data)
pred.nn <- pred.nn$net.result
# Calculate neural network error
SSE <- mean((train.yy-pred.nn)^2)
SST <- mean((train.yy-mean(train.yy))^2)
Rsqu.nn <- 1-SSE/SST
RMSE.nn <- sqrt(SSE)
# Forecast neural network
nn.test.data <- matrix(test.xx[,-diurnal.terms], nrow=1)
colnames(nn.test.data) <- paste0("response.",1:length(test.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.test.data)
pred.nn <- pred.nn$net.result
testing.results <- data.frame("Test.time"= test.index,
"Actual.forecast" = as.numeric(test.yy),
"NN.forecast" = as.numeric(pred.nn),
"Training.R2.NN" = as.numeric(Rsqu.nn),
"Training.RMSE.NN" = as.numeric(RMSE.nn))
return(testing.results)
# if(i==1) all.results <- testing.results
# if(i>1) all.results <- rbind(all.results, testing.results)
}
# for(i in 1:(total.obs-2*f-training.obs+1)) { # Faster to do this in an lapply loop
# testing.results <- do.call("rbind", lapply(1:(total.obs-2*f-training.obs+1), function(i) {
testing.results <- foreach(i=1:(total.obs-2*f-training.obs+1),
.combine = 'rbind', .packages = c("xts","stats","neuralnet")) %dopar% {
# Setup training and testing data for model
current.obs <- training.obs+f+i-1
train.xx <- matrix(as.numeric(obj.data[i:(current.obs-f),]), nrow=length(i:(current.obs-f)), byrow=FALSE)
train.yy <- matrix(as.numeric(obj.data[(i+f):current.obs, predict.col]), nrow=length(i:(current.obs-f)), byrow=FALSE)
n <- intersect(which(!is.na(train.yy)), which(!apply(train.xx,1,anyNA)))
train.xx <- train.xx[n,]
train.yy <- train.yy[n,]
test.xx <- matrix(as.numeric(obj.data[current.obs,]), nrow=1, byrow=FALSE)
test.yy <- matrix(as.numeric(obj.data[(current.obs+f), predict.col]), nrow=1, byrow=FALSE)
test.index <- index(obj.data[current.obs,])
# Train neural network model
diurnal.terms <- c(grep("cos", colnames(obj.data)), grep("sin", colnames(obj.data)))
nn.train.data <- cbind(train.yy, train.xx[,-diurnal.terms])
colnames(nn.train.data)[2:ncol(nn.train.data)] <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
fmla <- as.formula(paste0("train.yy ~ ",
paste(paste0("response.",1:ncol(train.xx[,-diurnal.terms])),collapse=" + ")))
nn <- neuralnet(fmla, data=nn.train.data, hidden=3,
act.fct = "logistic",
linear.output = TRUE,
stepmax=1e+06)
nn.train.data <- train.xx[,-diurnal.terms]
colnames(nn.train.data) <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.train.data)
pred.nn <- pred.nn$net.result
# Calculate neural network error
SSE <- mean((train.yy-pred.nn)^2)
SST <- mean((train.yy-mean(train.yy))^2)
Rsqu.nn <- 1-SSE/SST
RMSE.nn <- sqrt(SSE)
# Forecast neural network
nn.test.data <- matrix(test.xx[,-diurnal.terms], nrow=1)
colnames(nn.test.data) <- paste0("response.",1:length(test.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.test.data)
pred.nn <- pred.nn$net.result
testing.results <- data.frame("Test.time"= test.index,
"Actual.forecast" = as.numeric(test.yy),
"NN.forecast" = as.numeric(pred.nn),
"Training.R2.NN" = as.numeric(Rsqu.nn),
"Training.RMSE.NN" = as.numeric(RMSE.nn))
return(testing.results)
# if(i==1) all.results <- testing.results
# if(i>1) all.results <- rbind(all.results, testing.results)
}
?foreach
getwd()
setwd("C:\\Users\\R-Compiler\\Documents\\KNewhart\\LIFT_2019\\R Code")
##### Only NN
# Import data
{
library(xts)
library(readxl)
# This chunk takes upwards of an hour to execute, load "raw_data.RData" if avalible
if("raw_data.RData" %in% list.files(path="data")) {
load(file="data/raw_data.RData")
obj.list <- c("ab3_do",
"ab3_3.5",
"ab3_4.0",
"ab3_4.0_300",
"ab3_4.0_300_plus")
} else {
import_data <- "do"
source("src/import_data.R") # ab3_do
import_data <- "3.5 mg/L 90"
source("src/import_data.R") # ab3_3.5
import_data <- "4.0 mg/L 90"
source("src/import_data.R") # ab3_4.0
import_data <- "4.0 mg/L 300"
source("src/import_data.R") # ab3_4.0_300
source("src/import_historian_data.R") # ab3_4.0_300_plus
obj.list <- c("ab3_do",
"ab3_3.5",
"ab3_4.0",
"ab3_4.0_300",
"ab3_4.0_300_plus")
save(list=obj.list, file="data/raw_data.RData")
}
# Homogenize and shorten column names
source("src/col_name_fix.R")
# Preserve the original
real.data <- lapply(obj.list, function(x) get(x))
names(real.data) <- c("DO", "ABAC 3.5", "ABAC 4.0, 90s", "ABAC 4.0, 300s", "ABAC 4.0, 300s plus")
# Make column names, variable names
for(x in obj.list) {
data <- get(x)
colnames(data) <- make.names(colnames(data))
assign(x, data)
}
# Add missing timestamps
for(x in obj.list) {
time.intervals <- sapply(2:nrow(get(x)), function(i) difftime(index(get(x))[i], index(get(x))[i-1], units="mins"))
if(length(which(time.intervals != 5)) > 0) {
for(i in 1:length(which(time.intervals != 5))){
start.time <- index(get(x))[which(time.intervals != 5)[i]]
end.time <- index(get(x))[which(time.intervals != 5)[i]+1]
new.times <- seq(start.time, end.time, by=5*60)
new.x <- merge(get(x), new.times[2:(length(new.times)-1)], fill=NA)
assign(x, new.x)
}
}
}
# Equalize number of observations in each dataset
lapply(obj.list,
function(x) assign(x, get(x)[(nrow(get(x)) - min(sapply(obj.list, function(x) nrow(get(x))))):nrow(get(x)),],
envir = .GlobalEnv))
fixed.data <- lapply(obj.list, function(x) get(x))
}
library(dplyr)
# install.packages("keras")
library(keras)
library(parallel)
library(doSNOW)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
obj.data <- fixed.data[[1]] # DO dataset
load("results/all-days-ab3_do.RData")
all.data <- data.matrix(obj.data)
all.data <- all.data[,-which(colnames(all.data) == "Z9.DO")]
predict.col <- which(colnames(all.data) == "Z7.NH4")
#### JUST NN ####
all.days.ann.r <- list()
all.days.rnn.r <- list()
days <- 1
# Convert timestamps to runtime and project onto a unit circle
t <- 1440
time.stamps <- difftime(rownames(all.data), rownames(all.data)[1], units = "mins")
time.stamps <- as.numeric(time.stamps) %% t # Cycles are constructed
time.stamps <- (time.stamps*360/t)*pi/180 # Cycles of minutes are converted to radians
names(time.stamps) <- "time.stamps"
cos.x <- cos(time.stamps)
names(cos.x) <- "cos.x"
sin.x <- sin(time.stamps)
names(sin.x) <- "sin.x"
cos.2x <- cos(2*time.stamps)
names(cos.2x) <- "cos.2x"
sin.2x <- sin(2*time.stamps)
names(sin.2x) <- "sin.2x"
cos.3x <- cos(3*time.stamps)
names(cos.3x) <- "cos.3x"
sin.3x <- sin(3*time.stamps)
names(sin.3x) <- "sin.3x"
cos.4x <- cos(4*time.stamps)
names(cos.4x) <- "cos.4x"
sin.4x <- sin(4*time.stamps)
names(sin.4x) <- "sin.4x"
cos.5x <- cos(5*time.stamps)
names(cos.5x) <- "cos.5x"
sin.5x <- sin(5*time.stamps)
names(sin.5x) <- "sin.5x"
cos.6x <- cos(6*time.stamps)
names(cos.6x) <- "cos.6x"
sin.6x <- sin(6*time.stamps)
names(sin.6x) <- "sin.6x"
all.data <- cbind(all.data, cos.x, sin.x, cos.2x, sin.2x,
cos.3x, sin.3x, cos.4x, sin.4x, cos.5x, sin.5x, cos.6x, sin.6x)
days <- 1
lookback <- days*24*60/5 # Observations will go back 'days' at the 5 min interval
forecast.horizon <- seq(1,15) # 5-75 min forecast intervals
all.horizons.ann.r <- list()
all.horizons.rnn.r <- list()
delay <- forecast.horizon[1] # Targets will be some forecast horizon (in observations) into the future
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK", outfile="")
# register the cluster
registerDoSNOW(cluster)
pb <- txtProgressBar(min=0, max = length(iterations), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
# iterations <- seq(1,(nrow(all.data)-(lookback+delay)))
iterations <- seq((6*24*60/5-lookback)+1, nrow(all.data)-(lookback+delay),by=1)
pb <- txtProgressBar(min=0, max = length(iterations), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
print(paste("Starting RNN:", days, delay))
i
iterations
iterations <- iterations[1:10]
# results <- foreach(i=seq(1,(nrow(all.data)-(lookback+delay)),by=10), .combine="rbind", .packages=c("keras")) %dopar% {
results <- foreach(i=iterations, .combine="rbind", .packages=c("keras", "glmnet"), .options.snow = opts) %dopar% {
# Create model, add layers, and compile the model
model <- keras_model_sequential() %>%
# layer_dense(units=round((ncol(all.data)-1)*2/3), input_shape=c(NULL, ncol(all.data)+1)) %>%
layer_gru(units=round(ncol(all.data)*2/3),
# dropout = 0.1, # worse rmse
# recurrent_dropout = 0.1, # worse rmse
return_sequences = TRUE,
input_shape=list(ncol(all.data)+1,1)) %>%
layer_gru(units=ncol(all.data)+1) %>% # attemped to use dropout, worse r2 and rmse
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
train.x <- scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)
train.y <- scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]
# Train model when lambda=0 (initial parameter estimate)
mod.ridge <- cv.glmnet(train.x,train.y,alpha=0)
predict.mod.ridge <- predict(mod.ridge, newx=train.x)
# Adaptive lasso
w3 <- 1/abs(matrix(coef(mod.ridge, s=mod.ridge$lambda.min)[, 1][-1]))^1
set.seed(Sys.time())
mod.adaptive <- cv.glmnet(train.x,train.y,alpha=1,penalty.factor=w3)
pred.adapt <- predict(mod.adaptive,newx=train.x, s='lambda.1se')
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
pred.mean <- mean(pred.adapt)
pred.sd <- sd(pred.adapt)
train.x <- simplify2array(list(cbind(scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd),
scale(pred.adapt, center=pred.mean, scale = pred.sd), deparse.level = 0)))
train.y <- simplify2array(list(scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]))
history <- model %>% fit(
x=train.x,
y=train.y,
batch_size=1,
epochs=20
)
validation <- model %>% predict(
x=train.x,
batch_size=1
)
r2 <- cor(validation, train.y)^2;r2
# test.x <- simplify2array(list(scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)))
test.x <- scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)
# Forecast
pred.adapt <- predict(mod.adaptive,newx=test.x, s='lambda.1se')
pred.adapt <- scale(pred.adapt, center=pred.mean, scale = pred.sd)
prediction <- model %>% predict(
x=simplify2array(list(cbind(test.x, pred.adapt))),
batch_size=1
)
prediction <- prediction*train.sd[predict.col]+train.mean[predict.col]
actual <- all.data[(lookback+i+delay),predict.col]
e <- abs(actual-prediction);ed
# r2;e;past.test
return(data.frame(rownames(all.data)[(lookback+i)], actual, prediction, r2, e))
}
i <- 2
# Create model, add layers, and compile the model
model <- keras_model_sequential() %>%
# layer_dense(units=round((ncol(all.data)-1)*2/3), input_shape=c(NULL, ncol(all.data)+1)) %>%
layer_gru(units=round(ncol(all.data)*2/3),
# dropout = 0.1, # worse rmse
# recurrent_dropout = 0.1, # worse rmse
return_sequences = TRUE,
input_shape=list(ncol(all.data)+1,1)) %>%
layer_gru(units=ncol(all.data)+1) %>% # attemped to use dropout, worse r2 and rmse
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
train.x <- scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)
train.y <- scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]
# Train model when lambda=0 (initial parameter estimate)
mod.ridge <- cv.glmnet(train.x,train.y,alpha=0)
library(glmnet)
# Train model when lambda=0 (initial parameter estimate)
mod.ridge <- cv.glmnet(train.x,train.y,alpha=0)
predict.mod.ridge <- predict(mod.ridge, newx=train.x)
# Adaptive lasso
w3 <- 1/abs(matrix(coef(mod.ridge, s=mod.ridge$lambda.min)[, 1][-1]))^1
set.seed(Sys.time())
mod.adaptive <- cv.glmnet(train.x,train.y,alpha=1,penalty.factor=w3)
pred.adapt <- predict(mod.adaptive,newx=train.x, s='lambda.1se')
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
pred.mean <- mean(pred.adapt)
pred.sd <- sd(pred.adapt)
train.x <- simplify2array(list(cbind(scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd),
scale(pred.adapt, center=pred.mean, scale = pred.sd), deparse.level = 0)))
train.y <- simplify2array(list(scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]))
history <- model %>% fit(
x=train.x,
y=train.y,
batch_size=1,
epochs=20
)
validation <- model %>% predict(
x=train.x,
batch_size=1
)
r2 <- cor(validation, train.y)^2;r2
# test.x <- simplify2array(list(scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)))
test.x <- scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)
# Forecast
pred.adapt <- predict(mod.adaptive,newx=test.x, s='lambda.1se')
pred.adapt <- scale(pred.adapt, center=pred.mean, scale = pred.sd)
prediction <- model %>% predict(
x=simplify2array(list(cbind(test.x, pred.adapt))),
batch_size=1
)
prediction <- prediction*train.sd[predict.col]+train.mean[predict.col]
actual <- all.data[(lookback+i+delay),predict.col]
e <- abs(actual-prediction);ed
iterations
pb <- txtProgressBar(min=0, max = length(iterations), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
# results <- foreach(i=seq(1,(nrow(all.data)-(lookback+delay)),by=10), .combine="rbind", .packages=c("keras")) %dopar% {
results <- foreach(i=iterations, .combine="rbind", .packages=c("keras", "glmnet"), .options.snow = opts) %dopar% {
# Create model, add layers, and compile the model
model <- keras_model_sequential() %>%
# layer_dense(units=round((ncol(all.data)-1)*2/3), input_shape=c(NULL, ncol(all.data)+1)) %>%
layer_gru(units=round(ncol(all.data)*2/3),
# dropout = 0.1, # worse rmse
# recurrent_dropout = 0.1, # worse rmse
return_sequences = TRUE,
input_shape=list(ncol(all.data)+1,1)) %>%
layer_gru(units=ncol(all.data)+1) %>% # attemped to use dropout, worse r2 and rmse
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
train.x <- scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)
train.y <- scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]
# Train model when lambda=0 (initial parameter estimate)
mod.ridge <- cv.glmnet(train.x,train.y,alpha=0)
predict.mod.ridge <- predict(mod.ridge, newx=train.x)
# Adaptive lasso
w3 <- 1/abs(matrix(coef(mod.ridge, s=mod.ridge$lambda.min)[, 1][-1]))^1
set.seed(Sys.time())
mod.adaptive <- cv.glmnet(train.x,train.y,alpha=1,penalty.factor=w3)
pred.adapt <- predict(mod.adaptive,newx=train.x, s='lambda.1se')
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
pred.mean <- mean(pred.adapt)
pred.sd <- sd(pred.adapt)
train.x <- simplify2array(list(cbind(scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd),
scale(pred.adapt, center=pred.mean, scale = pred.sd), deparse.level = 0)))
train.y <- simplify2array(list(scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]))
history <- model %>% fit(
x=train.x,
y=train.y,
batch_size=1,
epochs=20
)
validation <- model %>% predict(
x=train.x,
batch_size=1
)
r2 <- cor(validation, train.y)^2
# test.x <- simplify2array(list(scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)))
test.x <- scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)
# Forecast
pred.adapt <- predict(mod.adaptive,newx=test.x, s='lambda.1se')
pred.adapt <- scale(pred.adapt, center=pred.mean, scale = pred.sd)
prediction <- model %>% predict(
x=simplify2array(list(cbind(test.x, pred.adapt))),
batch_size=1
)
prediction <- prediction*train.sd[predict.col]+train.mean[predict.col]
actual <- all.data[(lookback+i+delay),predict.col]
e <- abs(actual-prediction)
# r2;e;past.test
return(data.frame(rownames(all.data)[(lookback+i)], actual, prediction, r2, e))
}
results
