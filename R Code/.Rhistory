}
# for(i in 1:(total.obs-2*f-training.obs+1)) { # Faster to do this in an lapply loop
# testing.results <- do.call("rbind", lapply(1:(total.obs-2*f-training.obs+1), function(i) {
testing.results <- foreach(i=1:(total.obs-2*f-training.obs+1),
.combine = 'rbind', .packages = c("xts","stats","neuralnet")) %dopar% {
# Setup training and testing data for model
current.obs <- training.obs+f+i-1
train.xx <- matrix(as.numeric(obj.data[i:(current.obs-f),]), nrow=length(i:(current.obs-f)), byrow=FALSE)
train.yy <- matrix(as.numeric(obj.data[(i+f):current.obs, predict.col]), nrow=length(i:(current.obs-f)), byrow=FALSE)
n <- intersect(which(!is.na(train.yy)), which(!apply(train.xx,1,anyNA)))
train.xx <- train.xx[n,]
train.yy <- train.yy[n,]
test.xx <- matrix(as.numeric(obj.data[current.obs,]), nrow=1, byrow=FALSE)
test.yy <- matrix(as.numeric(obj.data[(current.obs+f), predict.col]), nrow=1, byrow=FALSE)
test.index <- index(obj.data[current.obs,])
# Train neural network model
diurnal.terms <- c(grep("cos", colnames(obj.data)), grep("sin", colnames(obj.data)))
nn.train.data <- cbind(train.yy, train.xx[,-diurnal.terms])
colnames(nn.train.data)[2:ncol(nn.train.data)] <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
fmla <- as.formula(paste0("train.yy ~ ",
paste(paste0("response.",1:ncol(train.xx[,-diurnal.terms])),collapse=" + ")))
nn <- neuralnet(fmla, data=nn.train.data, hidden=3,
act.fct = "logistic",
linear.output = TRUE,
stepmax=1e+06)
nn.train.data <- train.xx[,-diurnal.terms]
colnames(nn.train.data) <- paste0("response.",1:ncol(train.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.train.data)
pred.nn <- pred.nn$net.result
# Calculate neural network error
SSE <- mean((train.yy-pred.nn)^2)
SST <- mean((train.yy-mean(train.yy))^2)
Rsqu.nn <- 1-SSE/SST
RMSE.nn <- sqrt(SSE)
# Forecast neural network
nn.test.data <- matrix(test.xx[,-diurnal.terms], nrow=1)
colnames(nn.test.data) <- paste0("response.",1:length(test.xx[,-diurnal.terms]))
pred.nn <- compute(nn, nn.test.data)
pred.nn <- pred.nn$net.result
testing.results <- data.frame("Test.time"= test.index,
"Actual.forecast" = as.numeric(test.yy),
"NN.forecast" = as.numeric(pred.nn),
"Training.R2.NN" = as.numeric(Rsqu.nn),
"Training.RMSE.NN" = as.numeric(RMSE.nn))
return(testing.results)
# if(i==1) all.results <- testing.results
# if(i>1) all.results <- rbind(all.results, testing.results)
}
?foreach
list.files("results/",pattern="all-days-")
setwd("C:\\Users\\R-Compiler\\Documents\\KNewhart\\LIFT_2019\\R Code")
list.files("results/",pattern="all-days-")
load(all-days-ab3_do.RData)
load("results/all-days-ab3_do.RData")
View(all.days.ab3_do)
View(all.data)
{
library(xts)
library(readxl)
# This chunk takes upwards of an hour to execute, load "raw_data.RData" if avalible
if("raw_data.RData" %in% list.files(path="data")) {
load(file="data/raw_data.RData")
obj.list <- c("ab3_do",
"ab3_3.5",
"ab3_4.0",
"ab3_4.0_300",
"ab3_4.0_300_plus")
} else {
import_data <- "do"
source("src/import_data.R") # ab3_do
import_data <- "3.5 mg/L 90"
source("src/import_data.R") # ab3_3.5
import_data <- "4.0 mg/L 90"
source("src/import_data.R") # ab3_4.0
import_data <- "4.0 mg/L 300"
source("src/import_data.R") # ab3_4.0_300
source("src/import_historian_data.R") # ab3_4.0_300_plus
obj.list <- c("ab3_do",
"ab3_3.5",
"ab3_4.0",
"ab3_4.0_300",
"ab3_4.0_300_plus")
save(list=obj.list, file="data/raw_data.RData")
}
# Homogenize and shorten column names
source("src/col_name_fix.R")
# Preserve the original
real.data <- lapply(obj.list, function(x) get(x))
names(real.data) <- c("DO", "ABAC 3.5", "ABAC 4.0, 90s", "ABAC 4.0, 300s", "ABAC 4.0, 300s plus")
# Make column names, variable names
for(x in obj.list) {
data <- get(x)
colnames(data) <- make.names(colnames(data))
assign(x, data)
}
# Add missing timestamps
for(x in obj.list) {
time.intervals <- sapply(2:nrow(get(x)), function(i) difftime(index(get(x))[i], index(get(x))[i-1], units="mins"))
if(length(which(time.intervals != 5)) > 0) {
for(i in 1:length(which(time.intervals != 5))){
start.time <- index(get(x))[which(time.intervals != 5)[i]]
end.time <- index(get(x))[which(time.intervals != 5)[i]+1]
new.times <- seq(start.time, end.time, by=5*60)
new.x <- merge(get(x), new.times[2:(length(new.times)-1)], fill=NA)
assign(x, new.x)
}
}
}
# Equalize number of observations in each dataset
lapply(obj.list,
function(x) assign(x, get(x)[(nrow(get(x)) - min(sapply(obj.list, function(x) nrow(get(x))))):nrow(get(x)),],
envir = .GlobalEnv))
fixed.data <- lapply(obj.list, function(x) get(x))
}
obj.data <- fixed.data[[1]] # DO dataset
load("results/all-days-ab3_do.RData")
all.data <- data.matrix(obj.data)
all.data <- all.data[,-which(colnames(all.data) == "Z9.DO")]
predict.col <- which(colnames(all.data) == "Z7.NH4")
View(all.data)
View(ab3_do)
View(all.days.ab3_do)
all.days.ab3_do[[days]][[forecast.horizon]]
all.days.ab3_do[[1]][[11]]
colnames(all.days.ab3_do[[1]][[11]])
colnames(all.days.ab3_do[[1]][[11]][,3])
all.days.ab3_do[[1]][[11]][,3]
all.days.ab3_do[[1]][[11]][,c(1,3)]
all.data
obj.data <- fixed.data[[1]] # DO dataset
load("results/all-days-ab3_do.RData")
all.data <- data.matrix(obj.data)
all.data <- all.data[,-which(colnames(all.data) == "Z9.DO")]
predict.col <- which(colnames(all.data) == "Z7.NH4")
all.days.ann <- list()
all.days.ann.r <- list()
all.days.rnn <- list()
days <- 1
lookback <- days*24*60/5 # Observations will go back 'days' at the 5 min interval
forecast.horizon <- seq(1,15) # 5-75 min forecast intervals
all.horizons.ann <- list()
all.horizons.ann.r <- list()
all.horizons.rnn <- list()
delay <- forecast.horizon[1] # Targets will be some forecast horizon (in observations) into the future
# iterations <- seq(1,(nrow(all.data)-(lookback+delay)))
iterations <- seq((6*24*60/5-lookback)+1, nrow(all.data)-(lookback+delay),by=1)
iterations[1]
i:(lookback+i-1)
i <- iterations[1]
i:(lookback+i-1)
View(all.data[i:(lookback+i-1),])
View(all.days.ab3_do[[days]][[delay]])
# results <- foreach(i=1:5, .combine="rbind", .packages=c("keras")) %dopar% {
all.data.r <- xts(all.days.ab3_do[[days]][[delay]][,3], order.by = all.days.ab3_do[[days]][[delay]][,1])
all.data.r
index(all.data.r)
index(all.data)
dimnames(all.data)[[1]]
sapply(dimnames(all.data)[[1]], function(x) which(x==all.days.ab3_do[[days]][[delay]][,1]))
unlist(sapply(dimnames(all.data)[[1]], function(x) which(x==all.days.ab3_do[[days]][[delay]][,1])))
as.numeric(unlist(sapply(dimnames(all.data)[[1]], function(x) which(x==all.days.ab3_do[[days]][[delay]][,1]))))
dimnames(all.data)[[1]][1]
all.days.ab3_do[[days]][[delay]][1,1]
all.data.r <- cbind(all.data, data.matrix(all.days.ab3_do[[days]][[delay]][r.index,3]))
# results <- foreach(i=1:5, .combine="rbind", .packages=c("keras")) %dopar% {
r.index <- as.numeric(unlist(sapply(dimnames(all.data)[[1]], function(x) which(x==all.days.ab3_do[[days]][[delay]][,1]))))
all.data.r <- cbind(all.data, data.matrix(all.days.ab3_do[[days]][[delay]][r.index,3]))
length(r.index)
nrow(all.data)
length(dimnames(all.data)[1])
length(dimnames(all.data)[[1]])
i:(lookback+i-1)
r.index <- as.numeric(unlist(sapply(dimnames(all.data[i:(lookback+i-1),])[[1]], function(x) which(x==all.days.ab3_do[[days]][[delay]][,1]))))
train.data <- cbind(all.data[i:(lookback+i-1),], all.days.ab3_do[[days]][[delay]][r.index,3])
train.data
View(train.data)
for(days in 1:6) {
lookback <- days*24*60/5 # Observations will go back 'days' at the 5 min interval
forecast.horizon <- seq(1,15) # 5-75 min forecast intervals
all.horizons.ann <- list()
all.horizons.rnn <- list()
delay <- forecast.horizon[1] # Targets will be some forecast horizon (in observations) into the future
for(delay in forecast.horizon) {
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK", outfile="")
# register the cluster
registerDoSNOW(cluster)
# iterations <- seq(1,(nrow(all.data)-(lookback+delay)))
iterations <- seq((6*24*60/5-lookback)+1, nrow(all.data)-(lookback+delay),by=1)
pb <- txtProgressBar(min=0, max = length(iterations), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
print(paste("Starting ANN:", days, delay))
# ANN
results <- foreach(i=iterations, .combine="rbind", .packages=c("keras"), .options.snow = opts) %dopar% {
# results <- foreach(i=1:5, .combine="rbind", .packages=c("keras")) %dopar% {
# Create model, add layers, and compile the model
model <- keras_model_sequential() %>%
layer_dense(units=round((ncol(all.data))*2/3), input_shape=c(NULL, ncol(all.data))) %>%
# layer_gru(units=round((ncol(all.data)-1)*2/3),
#           # dropout = 0.2, recurrent_dropout = 0.2,
#           # return_sequences = TRUE,
#           input_shape=list(ncol(all.data),1)) %>%
# layer_gru(units=round((ncol(all.data)-1)*2/3)) %>%
layer_dense(units=ncol(all.data)) %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
# train.x <- simplify2array(list(scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)))
train.x <- scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)
# train.y <- simplify2array(list(scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]))
train.y <- scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]
history <- model %>% fit(
x=train.x,
y=train.y,
batch_size=1,
epochs=20
)
validation <- model %>% predict(
x=train.x,
batch_size=1
)
r2 <- cor(validation, train.y)^2;r2
# test.x <- simplify2array(list(scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)))
test.x <- scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)
prediction <- model %>% predict(
x=test.x,
batch_size=1
)
prediction <- prediction*train.sd[predict.col]+train.mean[predict.col]
actual <- all.data[(lookback+i+delay),predict.col]
e <- abs(actual-prediction);e
return(data.frame(rownames(all.data)[(lookback+i)], actual, prediction, r2, e))
}
close(pb)
stopCluster(cluster)
all.horizons.ann[[length(all.horizons.ann)+1]] <- results
# print(paste("Completed",days,"days and", delay*5,"min forecast at", Sys.time()))
rm(results)
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK", outfile="")
# register the cluster
registerDoSNOW(cluster)
pb <- txtProgressBar(min=0, max = length(iterations), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
#RNN
print(paste("Starting RNN:", days, delay))
# results <- foreach(i=seq(1,(nrow(all.data)-(lookback+delay)),by=10), .combine="rbind", .packages=c("keras")) %dopar% {
results <- foreach(i=iterations, .combine="rbind", .packages=c("keras"), .options.snow = opts) %dopar% {
# Create model, add layers, and compile the model
model <- keras_model_sequential() %>%
# layer_dense(units=round((ncol(all.data)-1)*2/3), input_shape=c(NULL, ncol(all.data))) %>%
layer_gru(units=round(ncol(all.data)*2/3),
# dropout = 0.1, # worse rmse
# recurrent_dropout = 0.1, # worse rmse
return_sequences = TRUE,
input_shape=list(ncol(all.data),1)) %>%
layer_gru(units=ncol(all.data)) %>% # attemped to use dropout, worse r2 and rmse
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
train.x <- simplify2array(list(scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)))
train.y <- simplify2array(list(scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]))
history <- model %>% fit(
x=train.x,
y=train.y,
batch_size=1,
epochs=20
)
validation <- model %>% predict(
x=train.x,
batch_size=1
)
r2 <- cor(validation, train.y)^2;r2
prediction <- model %>% predict(
x=simplify2array(list(scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd))),
batch_size=1
)
prediction <- prediction*train.sd[predict.col]+train.mean[predict.col]
actual <- all.data[(lookback+i+delay),predict.col]
e <- abs(actual-prediction);e
# r2;e;past.test
return(data.frame(rownames(all.data)[(lookback+i)], actual, prediction, r2, e))
}
close(pb)
stopCluster(cluster)
all.horizons.rnn[[length(all.horizons.rnn)+1]] <- results
rm(results)
}
all.days.ann[[length(all.days.ann)+1]] <- all.horizons.ann
all.days.rnn[[length(all.days.rnn)+1]] <- all.horizons.rnn
}
setwd("C:\\Users\\R-Compiler\\Documents\\KNewhart\\LIFT_2019\\R Code")
##### Only NN
# Import data
{
library(xts)
library(readxl)
# This chunk takes upwards of an hour to execute, load "raw_data.RData" if avalible
if("raw_data.RData" %in% list.files(path="data")) {
load(file="data/raw_data.RData")
obj.list <- c("ab3_do",
"ab3_3.5",
"ab3_4.0",
"ab3_4.0_300",
"ab3_4.0_300_plus")
} else {
import_data <- "do"
source("src/import_data.R") # ab3_do
import_data <- "3.5 mg/L 90"
source("src/import_data.R") # ab3_3.5
import_data <- "4.0 mg/L 90"
source("src/import_data.R") # ab3_4.0
import_data <- "4.0 mg/L 300"
source("src/import_data.R") # ab3_4.0_300
source("src/import_historian_data.R") # ab3_4.0_300_plus
obj.list <- c("ab3_do",
"ab3_3.5",
"ab3_4.0",
"ab3_4.0_300",
"ab3_4.0_300_plus")
save(list=obj.list, file="data/raw_data.RData")
}
# Homogenize and shorten column names
source("src/col_name_fix.R")
# Preserve the original
real.data <- lapply(obj.list, function(x) get(x))
names(real.data) <- c("DO", "ABAC 3.5", "ABAC 4.0, 90s", "ABAC 4.0, 300s", "ABAC 4.0, 300s plus")
# Make column names, variable names
for(x in obj.list) {
data <- get(x)
colnames(data) <- make.names(colnames(data))
assign(x, data)
}
# Add missing timestamps
for(x in obj.list) {
time.intervals <- sapply(2:nrow(get(x)), function(i) difftime(index(get(x))[i], index(get(x))[i-1], units="mins"))
if(length(which(time.intervals != 5)) > 0) {
for(i in 1:length(which(time.intervals != 5))){
start.time <- index(get(x))[which(time.intervals != 5)[i]]
end.time <- index(get(x))[which(time.intervals != 5)[i]+1]
new.times <- seq(start.time, end.time, by=5*60)
new.x <- merge(get(x), new.times[2:(length(new.times)-1)], fill=NA)
assign(x, new.x)
}
}
}
# Equalize number of observations in each dataset
lapply(obj.list,
function(x) assign(x, get(x)[(nrow(get(x)) - min(sapply(obj.list, function(x) nrow(get(x))))):nrow(get(x)),],
envir = .GlobalEnv))
fixed.data <- lapply(obj.list, function(x) get(x))
}
library(dplyr)
# install.packages("keras")
library(keras)
library(doSNOW)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
obj.data <- fixed.data[[1]] # DO dataset
load("results/all-days-ab3_do.RData")
all.data <- data.matrix(obj.data)
all.data <- all.data[,-which(colnames(all.data) == "Z9.DO")]
predict.col <- which(colnames(all.data) == "Z7.NH4")
all.days.ann <- list()
all.days.rnn <- list()
days <- 1
for(days in 1:6) {
lookback <- days*24*60/5 # Observations will go back 'days' at the 5 min interval
forecast.horizon <- seq(1,15) # 5-75 min forecast intervals
all.horizons.ann <- list()
all.horizons.rnn <- list()
delay <- forecast.horizon[1] # Targets will be some forecast horizon (in observations) into the future
for(delay in forecast.horizon) {
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK", outfile="")
# register the cluster
registerDoSNOW(cluster)
# iterations <- seq(1,(nrow(all.data)-(lookback+delay)))
iterations <- seq((6*24*60/5-lookback)+1, nrow(all.data)-(lookback+delay),by=1)
pb <- txtProgressBar(min=0, max = length(iterations), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
print(paste("Starting ANN:", days, delay))
# ANN
results <- foreach(i=iterations, .combine="rbind", .packages=c("keras"), .options.snow = opts) %dopar% {
# results <- foreach(i=1:5, .combine="rbind", .packages=c("keras")) %dopar% {
# Create model, add layers, and compile the model
model <- keras_model_sequential() %>%
layer_dense(units=round((ncol(all.data))*2/3), input_shape=c(NULL, ncol(all.data))) %>%
# layer_gru(units=round((ncol(all.data)-1)*2/3),
#           # dropout = 0.2, recurrent_dropout = 0.2,
#           # return_sequences = TRUE,
#           input_shape=list(ncol(all.data),1)) %>%
# layer_gru(units=round((ncol(all.data)-1)*2/3)) %>%
layer_dense(units=ncol(all.data)) %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
# train.x <- simplify2array(list(scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)))
train.x <- scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)
# train.y <- simplify2array(list(scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]))
train.y <- scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]
history <- model %>% fit(
x=train.x,
y=train.y,
batch_size=1,
epochs=20
)
validation <- model %>% predict(
x=train.x,
batch_size=1
)
r2 <- cor(validation, train.y)^2;r2
# test.x <- simplify2array(list(scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)))
test.x <- scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd)
prediction <- model %>% predict(
x=test.x,
batch_size=1
)
prediction <- prediction*train.sd[predict.col]+train.mean[predict.col]
actual <- all.data[(lookback+i+delay),predict.col]
e <- abs(actual-prediction);e
return(data.frame(rownames(all.data)[(lookback+i)], actual, prediction, r2, e))
}
close(pb)
stopCluster(cluster)
all.horizons.ann[[length(all.horizons.ann)+1]] <- results
# print(paste("Completed",days,"days and", delay*5,"min forecast at", Sys.time()))
rm(results)
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK", outfile="")
# register the cluster
registerDoSNOW(cluster)
pb <- txtProgressBar(min=0, max = length(iterations), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
#RNN
print(paste("Starting RNN:", days, delay))
# results <- foreach(i=seq(1,(nrow(all.data)-(lookback+delay)),by=10), .combine="rbind", .packages=c("keras")) %dopar% {
results <- foreach(i=iterations, .combine="rbind", .packages=c("keras"), .options.snow = opts) %dopar% {
# Create model, add layers, and compile the model
model <- keras_model_sequential() %>%
# layer_dense(units=round((ncol(all.data)-1)*2/3), input_shape=c(NULL, ncol(all.data))) %>%
layer_gru(units=round(ncol(all.data)*2/3),
# dropout = 0.1, # worse rmse
# recurrent_dropout = 0.1, # worse rmse
return_sequences = TRUE,
input_shape=list(ncol(all.data),1)) %>%
layer_gru(units=ncol(all.data)) %>% # attemped to use dropout, worse r2 and rmse
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "mae"
)
train.mean <- apply(all.data[i:(lookback+i-1),],2,mean)
train.sd <- apply(all.data[i:(lookback+i-1),],2,sd)
train.x <- simplify2array(list(scale(all.data[i:(lookback+i-1),], center=train.mean, scale=train.sd)))
train.y <- simplify2array(list(scale(all.data[(i+delay):(lookback+delay+i-1),], center=train.mean, scale=train.sd)[,predict.col]))
history <- model %>% fit(
x=train.x,
y=train.y,
batch_size=1,
epochs=20
)
validation <- model %>% predict(
x=train.x,
batch_size=1
)
r2 <- cor(validation, train.y)^2;r2
prediction <- model %>% predict(
x=simplify2array(list(scale(t(all.data[(lookback+i),]), center=train.mean, scale=train.sd))),
batch_size=1
)
prediction <- prediction*train.sd[predict.col]+train.mean[predict.col]
actual <- all.data[(lookback+i+delay),predict.col]
e <- abs(actual-prediction);e
# r2;e;past.test
return(data.frame(rownames(all.data)[(lookback+i)], actual, prediction, r2, e))
}
close(pb)
stopCluster(cluster)
all.horizons.rnn[[length(all.horizons.rnn)+1]] <- results
rm(results)
}
all.days.ann[[length(all.days.ann)+1]] <- all.horizons.ann
all.days.rnn[[length(all.days.rnn)+1]] <- all.horizons.rnn
}
save(all.days.ann, file="results/ann_2layer_do.RData")
save(all.days.rnn, file="results/rnn_2layer_do.RData")
# clean up a bit.
# invisible(gc); remove(nThreads); remove(cluster)
###### Residual NN
all.days.files <- sapply(list.files("results/",pattern="all-days-"), function(x) load(paste0("results/",x), envir=.GlobalEnv))
# Merge with all data and re-run
library(doSNOW)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
install.packages(c("callr", "curl", "data.table", "Deriv", "devtools", "digest", "feather", "glmnet", "haven", "hms", "htmltools", "knitr", "openxlsx", "pkgbuild", "purrr", "R.oo", "R6", "Rcpp", "rlang", "rmarkdown", "roxygen2", "rvest", "scales", "selectr", "sys", "testthat", "tidyr", "tidyverse", "tinytex", "whisker", "xfun", "zip"))
